{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16bed17e-f7ae-4af2-b9d8-95998e4cd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import timm\n",
    "from timm.models.vision_transformer import trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0af2cc0-2820-498b-9373-8ef71bad0e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5e2101-3c77-4a6e-bdcb-e88aebc1942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - img_size : int\n",
    "        - patch_size : int\n",
    "        - in_chans : int\n",
    "        - embed_dim : int\n",
    "\n",
    "    Internal Attributes:\n",
    "        - n_patches : int\n",
    "        - proj : nn.Conv2d\n",
    "            Conv for splitting into patches and embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        # NOTE(Abid): Conv2d is the hybrid architecture proposal of the paper,\n",
    "        #             the original version has a MLP instead.\n",
    "        self.proj = nn.Conv2d(\n",
    "                in_chans,\n",
    "                embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                - x : torch.Tensor\n",
    "                    Shape (batch, in_chans, img_size, img_size)\n",
    "\n",
    "            Return: torch.Tensor\n",
    "                    Shape (batch, n_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.proj(x) # Shape (batch, embed_dim, n_patches ** 0.5, n_patches ** 5)\n",
    "        x = x.flatten(2) # Shape (batch, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2) # Shape (batch, n_patches, embed_dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            - dim : int\n",
    "                input and output dim of per token features\n",
    "            - n_heads : int\n",
    "            - qkv_bias : bool\n",
    "            - attn_p : float\n",
    "                Dropout probability for qkv\n",
    "            - proj_p : float\n",
    "                Dropout probability for output\n",
    "\n",
    "        Attributes:\n",
    "            - scale : float\n",
    "                For scaled dot product\n",
    "            - qkv : nn.Linear\n",
    "            - proj : nn.Linear\n",
    "            - attn_drop, proj_drop : nn.Dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0, proj_p=0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                - x : torch.Tensor\n",
    "                      Shape (batch, n_patches + 1, dim)\n",
    "\n",
    "            Returns:\n",
    "                - out : torch.Tensor\n",
    "                        Shape (batch, n_patches + 1, dim)\n",
    "        \"\"\"\n",
    "        batch, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x) # Shape : (batch, n_patches + 1, 3 * dim)\n",
    "        qkv = qkv.reshape(batch, n_tokens, 3, self.n_heads, self.head_dim) # Shape : (batch, n_patches+1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # Shape : (3, batch, n_heads, n_patches+1, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1) # Shape : (batch, n_heads, head_dim, n_patches+1)\n",
    "        dot = (q @ k_t) * self.scale # Shape : (batch, n_heads, n_patches+1, n_patches+1)\n",
    "        attention = dot.softmax(dim=-1)\n",
    "        attention = self.attn_drop(attention)\n",
    "\n",
    "        weighted_avg = attention @ v # Shape : (batch, n_heads, n_patches+1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1, 2) # Shape : (batch, n_patches+1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2) # Shape : (batch, n_patches+1, dim)\n",
    "\n",
    "        x = self.proj(weighted_avg) # Shape : (batch, n_patches+1, dim)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            - in_features: int\n",
    "            - hidden_features : int\n",
    "            - out_features : int\n",
    "            - p : float\n",
    "                Dropout\n",
    "\n",
    "        Attributes:\n",
    "            - fc : nn.Linear\n",
    "            - act : nn.GELU\n",
    "                Activation\n",
    "            - fc2 : nn.Linear\n",
    "            - drop : nn.Dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                - x : torch.Tensor\n",
    "                      Shape (batch, n_patches +1, in_features)\n",
    "\n",
    "            Returns:\n",
    "                - torch.Tensor\n",
    "                  Shape (batch, n_patches +1, out_features)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.fc1(x) # Shape (batch, n_patches +1, hidden_features)\n",
    "        x = self.act(x) \n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x) # Shape (batch, n_patches +1, out_features)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            - dim : int\n",
    "                Embedding\n",
    "            - n_heads : int\n",
    "            - mlp_ratio : float\n",
    "                Determines the hidden dimension size of the MLP wrt dim\n",
    "            - qkv_bias : bool\n",
    "            - p, attn_p : float\n",
    "\n",
    "        Attributes:\n",
    "            - norm1, norm2 : LayerNorm\n",
    "            - attention : Attention\n",
    "            - mlp : MLP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0.0, attn_p=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.mha = MultiHeadAttention(\n",
    "                dim,\n",
    "                n_heads=n_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.ff = FeedForward(in_features=dim,\n",
    "                              hidden_features=hidden_features,\n",
    "                              out_features=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "                - x : torch.Tensor\n",
    "                      Shape (batch, n_patches + 1, dim)\n",
    "            Returns\n",
    "                - torch.Tensor\n",
    "                  Shape (batch, n_patches + 1, dim)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.mha(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            - img_size : int\n",
    "                Square image assumed\n",
    "            - patch_size: int\n",
    "            - in_chans : int\n",
    "            - n_classes : int\n",
    "            - embed_dim : int\n",
    "            - depth : int\n",
    "                Number of blocks\n",
    "            - mlp_ratio : float\n",
    "            - qkv_bias : bool\n",
    "            - p, attn_p : float\n",
    "\n",
    "        Attributes:\n",
    "            - patch_embed : PatchEmbed\n",
    "            - cls_token : nn.Parameter\n",
    "                Prepended parameter used for classification, the first one in order\n",
    "            - pos_emb : nn.Parameter\n",
    "            - pos_drop : nn.Dropout\n",
    "            - blocks: nn.ModuleList\n",
    "            - norm : nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=384, patch_size=16, in_chans=3, n_classes=1000, embed_dim=768,\n",
    "                 depth=12, n_heads=12, mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                in_chans=in_chans,\n",
    "                embed_dim=embed_dim,\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.Sequential(*[Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio,\n",
    "                                            qkv_bias=qkv_bias, p=p, attn_p=attn_p) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        # NOTE(Abid): According to the original paper, the MLP here should have a single hidden layer in training phase.\n",
    "        self.pred_head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                - x : torch.Tensor\n",
    "                      Shape (batch, in_chans, img_size, img_size)\n",
    "\n",
    "            Returns:\n",
    "                - logits : torch.Tensor\n",
    "                           Shape (batch, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        batch = x.shape[0]\n",
    "        x = self.patch_embed(x) # (batch, n_patches, embed_dim)\n",
    "        cls_token = self.cls_token.expand(batch, -1, -1) # Shape (batch, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1) # Shape (batch, n_patches + 1, embed_dim)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0] # Shape (batch, embed_dim) --> Only the first patch is used\n",
    "        x = self.pred_head(cls_token_final) # (batch, n_classes)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf55d286-4762-4df1-9f97-9b4009707b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeitTransformer(VisionTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_prefix_tokens = 2\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, kwargs.get('embed_dim')))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.patch_embed.n_patches + self.num_prefix_tokens, kwargs.get('embed_dim')))\n",
    "        self.head_dist = nn.Linear(kwargs.get('embed_dim'), kwargs.get(\"n_classes\")) if kwargs.get(\"n_classes\") > 0 else nn.Identity()\n",
    "        self.distilled_training = False\n",
    "        \n",
    "    def init_weights(self, mode=''):\n",
    "        trunc_normal_(self.dist_token, std=.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = torch.cat((\n",
    "            self.cls_token.expand(x.shape[0], -1, -1),\n",
    "            self.dist_token.expand(x.shape[0], -1, -1), x),\n",
    "            dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        x, x_dist = x[:, 0], x[:, 1]\n",
    "        x = self.pred_head(x)\n",
    "        x_dist = self.head_dist(x_dist)\n",
    "        return (x + x_dist) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bb61bda-f05b-438f-a9ac-a1c70932f661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token | cls_token\n",
      "pos_embed | pos_emb\n",
      "patch_embed.proj.weight | dist_token\n",
      "patch_embed.proj.bias | pos_embed\n",
      "blocks.0.norm1.weight | patch_embed.proj.weight\n",
      "blocks.0.norm1.bias | patch_embed.proj.bias\n",
      "blocks.0.attn.qkv.weight | blocks.0.norm1.weight\n",
      "blocks.0.attn.qkv.bias | blocks.0.norm1.bias\n",
      "blocks.0.attn.proj.weight | blocks.0.mha.qkv.weight\n",
      "blocks.0.attn.proj.bias | blocks.0.mha.qkv.bias\n",
      "blocks.0.norm2.weight | blocks.0.mha.proj.weight\n",
      "blocks.0.norm2.bias | blocks.0.mha.proj.bias\n",
      "blocks.0.mlp.fc1.weight | blocks.0.norm2.weight\n",
      "blocks.0.mlp.fc1.bias | blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc2.weight | blocks.0.ff.fc1.weight\n",
      "blocks.0.mlp.fc2.bias | blocks.0.ff.fc1.bias\n",
      "blocks.1.norm1.weight | blocks.0.ff.fc2.weight\n",
      "blocks.1.norm1.bias | blocks.0.ff.fc2.bias\n",
      "blocks.1.attn.qkv.weight | blocks.1.norm1.weight\n",
      "blocks.1.attn.qkv.bias | blocks.1.norm1.bias\n",
      "blocks.1.attn.proj.weight | blocks.1.mha.qkv.weight\n",
      "blocks.1.attn.proj.bias | blocks.1.mha.qkv.bias\n",
      "blocks.1.norm2.weight | blocks.1.mha.proj.weight\n",
      "blocks.1.norm2.bias | blocks.1.mha.proj.bias\n",
      "blocks.1.mlp.fc1.weight | blocks.1.norm2.weight\n",
      "blocks.1.mlp.fc1.bias | blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc2.weight | blocks.1.ff.fc1.weight\n",
      "blocks.1.mlp.fc2.bias | blocks.1.ff.fc1.bias\n",
      "blocks.2.norm1.weight | blocks.1.ff.fc2.weight\n",
      "blocks.2.norm1.bias | blocks.1.ff.fc2.bias\n",
      "blocks.2.attn.qkv.weight | blocks.2.norm1.weight\n",
      "blocks.2.attn.qkv.bias | blocks.2.norm1.bias\n",
      "blocks.2.attn.proj.weight | blocks.2.mha.qkv.weight\n",
      "blocks.2.attn.proj.bias | blocks.2.mha.qkv.bias\n",
      "blocks.2.norm2.weight | blocks.2.mha.proj.weight\n",
      "blocks.2.norm2.bias | blocks.2.mha.proj.bias\n",
      "blocks.2.mlp.fc1.weight | blocks.2.norm2.weight\n",
      "blocks.2.mlp.fc1.bias | blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc2.weight | blocks.2.ff.fc1.weight\n",
      "blocks.2.mlp.fc2.bias | blocks.2.ff.fc1.bias\n",
      "blocks.3.norm1.weight | blocks.2.ff.fc2.weight\n",
      "blocks.3.norm1.bias | blocks.2.ff.fc2.bias\n",
      "blocks.3.attn.qkv.weight | blocks.3.norm1.weight\n",
      "blocks.3.attn.qkv.bias | blocks.3.norm1.bias\n",
      "blocks.3.attn.proj.weight | blocks.3.mha.qkv.weight\n",
      "blocks.3.attn.proj.bias | blocks.3.mha.qkv.bias\n",
      "blocks.3.norm2.weight | blocks.3.mha.proj.weight\n",
      "blocks.3.norm2.bias | blocks.3.mha.proj.bias\n",
      "blocks.3.mlp.fc1.weight | blocks.3.norm2.weight\n",
      "blocks.3.mlp.fc1.bias | blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc2.weight | blocks.3.ff.fc1.weight\n",
      "blocks.3.mlp.fc2.bias | blocks.3.ff.fc1.bias\n",
      "blocks.4.norm1.weight | blocks.3.ff.fc2.weight\n",
      "blocks.4.norm1.bias | blocks.3.ff.fc2.bias\n",
      "blocks.4.attn.qkv.weight | blocks.4.norm1.weight\n",
      "blocks.4.attn.qkv.bias | blocks.4.norm1.bias\n",
      "blocks.4.attn.proj.weight | blocks.4.mha.qkv.weight\n",
      "blocks.4.attn.proj.bias | blocks.4.mha.qkv.bias\n",
      "blocks.4.norm2.weight | blocks.4.mha.proj.weight\n",
      "blocks.4.norm2.bias | blocks.4.mha.proj.bias\n",
      "blocks.4.mlp.fc1.weight | blocks.4.norm2.weight\n",
      "blocks.4.mlp.fc1.bias | blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc2.weight | blocks.4.ff.fc1.weight\n",
      "blocks.4.mlp.fc2.bias | blocks.4.ff.fc1.bias\n",
      "blocks.5.norm1.weight | blocks.4.ff.fc2.weight\n",
      "blocks.5.norm1.bias | blocks.4.ff.fc2.bias\n",
      "blocks.5.attn.qkv.weight | blocks.5.norm1.weight\n",
      "blocks.5.attn.qkv.bias | blocks.5.norm1.bias\n",
      "blocks.5.attn.proj.weight | blocks.5.mha.qkv.weight\n",
      "blocks.5.attn.proj.bias | blocks.5.mha.qkv.bias\n",
      "blocks.5.norm2.weight | blocks.5.mha.proj.weight\n",
      "blocks.5.norm2.bias | blocks.5.mha.proj.bias\n",
      "blocks.5.mlp.fc1.weight | blocks.5.norm2.weight\n",
      "blocks.5.mlp.fc1.bias | blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc2.weight | blocks.5.ff.fc1.weight\n",
      "blocks.5.mlp.fc2.bias | blocks.5.ff.fc1.bias\n",
      "blocks.6.norm1.weight | blocks.5.ff.fc2.weight\n",
      "blocks.6.norm1.bias | blocks.5.ff.fc2.bias\n",
      "blocks.6.attn.qkv.weight | blocks.6.norm1.weight\n",
      "blocks.6.attn.qkv.bias | blocks.6.norm1.bias\n",
      "blocks.6.attn.proj.weight | blocks.6.mha.qkv.weight\n",
      "blocks.6.attn.proj.bias | blocks.6.mha.qkv.bias\n",
      "blocks.6.norm2.weight | blocks.6.mha.proj.weight\n",
      "blocks.6.norm2.bias | blocks.6.mha.proj.bias\n",
      "blocks.6.mlp.fc1.weight | blocks.6.norm2.weight\n",
      "blocks.6.mlp.fc1.bias | blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc2.weight | blocks.6.ff.fc1.weight\n",
      "blocks.6.mlp.fc2.bias | blocks.6.ff.fc1.bias\n",
      "blocks.7.norm1.weight | blocks.6.ff.fc2.weight\n",
      "blocks.7.norm1.bias | blocks.6.ff.fc2.bias\n",
      "blocks.7.attn.qkv.weight | blocks.7.norm1.weight\n",
      "blocks.7.attn.qkv.bias | blocks.7.norm1.bias\n",
      "blocks.7.attn.proj.weight | blocks.7.mha.qkv.weight\n",
      "blocks.7.attn.proj.bias | blocks.7.mha.qkv.bias\n",
      "blocks.7.norm2.weight | blocks.7.mha.proj.weight\n",
      "blocks.7.norm2.bias | blocks.7.mha.proj.bias\n",
      "blocks.7.mlp.fc1.weight | blocks.7.norm2.weight\n",
      "blocks.7.mlp.fc1.bias | blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc2.weight | blocks.7.ff.fc1.weight\n",
      "blocks.7.mlp.fc2.bias | blocks.7.ff.fc1.bias\n",
      "blocks.8.norm1.weight | blocks.7.ff.fc2.weight\n",
      "blocks.8.norm1.bias | blocks.7.ff.fc2.bias\n",
      "blocks.8.attn.qkv.weight | blocks.8.norm1.weight\n",
      "blocks.8.attn.qkv.bias | blocks.8.norm1.bias\n",
      "blocks.8.attn.proj.weight | blocks.8.mha.qkv.weight\n",
      "blocks.8.attn.proj.bias | blocks.8.mha.qkv.bias\n",
      "blocks.8.norm2.weight | blocks.8.mha.proj.weight\n",
      "blocks.8.norm2.bias | blocks.8.mha.proj.bias\n",
      "blocks.8.mlp.fc1.weight | blocks.8.norm2.weight\n",
      "blocks.8.mlp.fc1.bias | blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc2.weight | blocks.8.ff.fc1.weight\n",
      "blocks.8.mlp.fc2.bias | blocks.8.ff.fc1.bias\n",
      "blocks.9.norm1.weight | blocks.8.ff.fc2.weight\n",
      "blocks.9.norm1.bias | blocks.8.ff.fc2.bias\n",
      "blocks.9.attn.qkv.weight | blocks.9.norm1.weight\n",
      "blocks.9.attn.qkv.bias | blocks.9.norm1.bias\n",
      "blocks.9.attn.proj.weight | blocks.9.mha.qkv.weight\n",
      "blocks.9.attn.proj.bias | blocks.9.mha.qkv.bias\n",
      "blocks.9.norm2.weight | blocks.9.mha.proj.weight\n",
      "blocks.9.norm2.bias | blocks.9.mha.proj.bias\n",
      "blocks.9.mlp.fc1.weight | blocks.9.norm2.weight\n",
      "blocks.9.mlp.fc1.bias | blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc2.weight | blocks.9.ff.fc1.weight\n",
      "blocks.9.mlp.fc2.bias | blocks.9.ff.fc1.bias\n",
      "blocks.10.norm1.weight | blocks.9.ff.fc2.weight\n",
      "blocks.10.norm1.bias | blocks.9.ff.fc2.bias\n",
      "blocks.10.attn.qkv.weight | blocks.10.norm1.weight\n",
      "blocks.10.attn.qkv.bias | blocks.10.norm1.bias\n",
      "blocks.10.attn.proj.weight | blocks.10.mha.qkv.weight\n",
      "blocks.10.attn.proj.bias | blocks.10.mha.qkv.bias\n",
      "blocks.10.norm2.weight | blocks.10.mha.proj.weight\n",
      "blocks.10.norm2.bias | blocks.10.mha.proj.bias\n",
      "blocks.10.mlp.fc1.weight | blocks.10.norm2.weight\n",
      "blocks.10.mlp.fc1.bias | blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc2.weight | blocks.10.ff.fc1.weight\n",
      "blocks.10.mlp.fc2.bias | blocks.10.ff.fc1.bias\n",
      "blocks.11.norm1.weight | blocks.10.ff.fc2.weight\n",
      "blocks.11.norm1.bias | blocks.10.ff.fc2.bias\n",
      "blocks.11.attn.qkv.weight | blocks.11.norm1.weight\n",
      "blocks.11.attn.qkv.bias | blocks.11.norm1.bias\n",
      "blocks.11.attn.proj.weight | blocks.11.mha.qkv.weight\n",
      "blocks.11.attn.proj.bias | blocks.11.mha.qkv.bias\n",
      "blocks.11.norm2.weight | blocks.11.mha.proj.weight\n",
      "blocks.11.norm2.bias | blocks.11.mha.proj.bias\n",
      "blocks.11.mlp.fc1.weight | blocks.11.norm2.weight\n",
      "blocks.11.mlp.fc1.bias | blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc2.weight | blocks.11.ff.fc1.weight\n",
      "blocks.11.mlp.fc2.bias | blocks.11.ff.fc1.bias\n",
      "norm.weight | blocks.11.ff.fc2.weight\n",
      "norm.bias | blocks.11.ff.fc2.bias\n",
      "head.weight | norm.weight\n",
      "head.bias | norm.bias\n"
     ]
    }
   ],
   "source": [
    "custom_config = {\n",
    "    \"n_classes\" : 1000,\n",
    "    \"img_size\" : 224,\n",
    "    \"patch_size\" : 16,\n",
    "    \"in_chans\" : 3,\n",
    "    \"embed_dim\" : 192,\n",
    "    \"depth\" : 12,\n",
    "    \"n_heads\" : 3,\n",
    "    \"qkv_bias\" : True,\n",
    "    \"mlp_ratio\" : 4\n",
    "}\n",
    "deit_model = DeitTransformer(**custom_config)\n",
    "deit_model.eval();\n",
    "\n",
    "model_official = timm.create_model(\"deit_tiny_patch16_224\", pretrained=True)\n",
    "\n",
    "for (n_o, p_o), (n_c, p_c) in zip (model_official.named_parameters(),\n",
    "                                   deit_model.named_parameters()):\n",
    "    # assert p_o.numel() == p_c.numel()\n",
    "    print(f\"{n_o} | {n_c}\")\n",
    "\n",
    "    # p_c.data[:] = p_o.data\n",
    "\n",
    "    # assert_tensors_equal(p_c.data, p_o.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed5bf7a9-febe-42cb-b574-41f512e84219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: porcupine, hedgehog                           --- 0.0052\n",
      "1: electric_fan, blower                          --- 0.0049\n",
      "2: loudspeaker, speaker, speaker_unit, loudspeaker_system, speaker_system --- 0.0047\n",
      "3: folding_chair                                 --- 0.0046\n",
      "4: window_screen                                 --- 0.0043\n",
      "5: joystick                                      --- 0.0042\n",
      "6: steel_arch_bridge                             --- 0.0041\n",
      "7: space_heater                                  --- 0.0040\n",
      "8: binoculars, field_glasses, opera_glasses      --- 0.0037\n",
      "9: mousetrap                                     --- 0.0037\n"
     ]
    }
   ],
   "source": [
    "def assert_tensors_equal(t1, t2):\n",
    "    a1, a2 = t1.detach().numpy(), t2.detach().numpy()\n",
    "\n",
    "# --------| Verifying the Model |----------\n",
    "inputs = torch.rand(1, 3, 224, 224)\n",
    "res_c = deit_model(inputs)\n",
    "res_o = model_official(inputs)\n",
    "\n",
    "#assert get_n_params(model_custom) == get_n_params(model_official)\n",
    "assert_tensors_equal(res_c, res_o)\n",
    "\n",
    "# torch.save(model_custom, \"vit_model.pth\")\n",
    "\n",
    "\n",
    "# ---------| Inference |----------\n",
    "from PIL import Image\n",
    "\n",
    "k = 10\n",
    "\n",
    "imagenet_labels = dict(enumerate(open(\"classes.txt\")))\n",
    "\n",
    "img = (np.array(Image.open(\"elephant.jpg\")) / 256)\n",
    "inputs = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(torch.float32) # Shape (1, in_chans, img_size, img_size)\n",
    "logits = deit_model(inputs) # Shape (1, n_classes)\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "top_probs, top_ixs = probs[0].topk(k)\n",
    "\n",
    "for i, (ix_, prob_) in enumerate(zip(top_ixs, top_probs)):\n",
    "    ix = ix_.item()\n",
    "    prob = prob_.item()\n",
    "    cls = imagenet_labels[ix].strip()\n",
    "    print(f\"{i}: {cls:<45} --- {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c9551-cea5-41ac-959b-27e026c75fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a705011-e93d-40e2-9319-8f7175f16e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
